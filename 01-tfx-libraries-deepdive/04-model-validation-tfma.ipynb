{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Validation with TFMA\n",
    "\n",
    "In this lab, we use [TensorFlow Model Analysis](https://www.tensorflow.org/tfx/guide/tfma) to assess the quality of the trained model. This lab covers the following:\n",
    "1. **Export** evaluation saved model\n",
    "2. Define **data slices** for analysis\n",
    "3. Generat **evaluation** the metrics\n",
    "4. **Visualize** results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q tensorflow-model-analysis\n",
    "# !jupyter nbextension enable --py widgetsnbextension\n",
    "# !jupyter nbextension install --py --symlink tensorflow_model_analysis\n",
    "# !jupyter nbextension enable --py tensorflow_model_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow.io as tf_io\n",
    "import tensorflow_transform as tft\n",
    "import tensorflow_model_analysis as tfma\n",
    "import tensorflow_data_validation as tfdv\n",
    "from tensorflow_transform.tf_metadata import schema_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKSPACE = 'workspace' # you can set to a GCS location\n",
    "RAW_SCHEMA_LOCATION = os.path.join(WORKSPACE, 'raw_schema.pbtxt')\n",
    "DATA_DIR = os.path.join(WORKSPACE, 'raw_data')\n",
    "TRANSFORM_ARTEFACTS_DIR = os.path.join(WORKSPACE, 'transform_artifacts')\n",
    "DATA_FILES = os.path.join(DATA_DIR,'*.csv')\n",
    "MODELS_DIR = os.path.join(WORKSPACE, 'models')\n",
    "MODEL_NAME = 'dnn_classifier'\n",
    "MODEL_DIR = os.path.join(MODELS_DIR, MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load TFT Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_output = tft.TFTransformOutput(TRANSFORM_ARTEFACTS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Export Evaluation Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADER = ['age', 'workclass', 'fnlwgt', 'education', 'education_num',\n",
    "          'marital_status', 'occupation', 'relationship', 'race', 'gender',\n",
    "          'capital_gain', 'capital_loss', 'hours_per_week',\n",
    "          'native_country', 'income_bracket']\n",
    "\n",
    "HEADER_DEFAULTS = [[0], [''], [0], [''], [0], [''], [''], [''], [''], [''],\n",
    "                   [0], [0], [0], [''], ['']]\n",
    "\n",
    "TARGET_FEATURE_NAME = 'income_bracket'\n",
    "TARGET_LABELS = [' <=50K', ' >50K']\n",
    "WEIGHT_COLUMN_NAME = 'fnlwgt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Implement eval_input_receiver_fn\n",
    "This function expect **raw** data interface, then it applies the **transformation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_input_receiver_fn():\n",
    "    \n",
    "    receiver_tensors = {'examples': tf.placeholder(dtype=tf.string, shape=[None])}\n",
    "    columns = tf.decode_csv(receiver_tensors['examples'], record_defaults=HEADER_DEFAULTS)\n",
    "    \n",
    "    features = dict(zip(HEADER, columns))\n",
    "    \n",
    "    for feature_name in features:\n",
    "        if features[feature_name].dtype == tf.int32:\n",
    "            features[feature_name] = tf.cast(features[feature_name], tf.int64)\n",
    "        features[feature_name] = tf.reshape(features[feature_name], (-1, 1))\n",
    "        \n",
    "    transformed_features = transform_output.transform_raw_features(features)\n",
    "    features.update(transformed_features)\n",
    "\n",
    "    return tfma.export.EvalInputReceiver(\n",
    "        features=features,\n",
    "        receiver_tensors=receiver_tensors,\n",
    "        labels=features[TARGET_FEATURE_NAME]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Export an evaluation saved model\n",
    "First, we load the estimator..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "class Parameters: pass\n",
    "estimator = joblib.load( './estimator.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_optimizer(initial_learning_rate, decay_steps):\n",
    "    learning_rate = tf.train.cosine_decay_restarts(\n",
    "        initial_learning_rate,\n",
    "        tf.train.get_global_step(),\n",
    "        first_decay_steps=50,\n",
    "        t_mul=2.0,\n",
    "        m_mul=1.0,\n",
    "        alpha=0.0,\n",
    "    )\n",
    "    \n",
    "    tf.summary.scalar('learning_rate', learning_rate)\n",
    "    return tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "def metric_fn(labels, predictions):\n",
    "    \n",
    "    metrics = {}\n",
    "    label_index = tf.contrib.lookup.index_table_from_tensor(tf.constant(TARGET_LABELS)).lookup(labels)\n",
    "    one_hot_labels = tf.one_hot(label_index, len(TARGET_LABELS))\n",
    "    \n",
    "    metrics['mirco_accuracy'] = tf.metrics.mean_per_class_accuracy(\n",
    "        labels=label_index,\n",
    "        predictions=predictions['class_ids'],\n",
    "        num_classes=2\n",
    "    )\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_FEATURE_NAME = 'income_bracket'\n",
    "TARGET_LABELS = [' <=50K', ' >50K']\n",
    "WEIGHT_COLUMN_NAME = 'fnlwgt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "eval_model_dir = os.path.join(MODEL_DIR, \"export/evaluate\")\n",
    "if tf_io.gfile.exists(eval_model_dir):\n",
    "    tf_io.gfile.rmtree(eval_model_dir)\n",
    "\n",
    "eval_model_dir = tfma.export.export_eval_savedmodel(\n",
    "        estimator=estimator,\n",
    "        export_dir_base=eval_model_dir,\n",
    "        eval_input_receiver_fn=eval_input_receiver_fn\n",
    ")\n",
    "\n",
    "eval_model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!saved_model_cli show --dir=${eval_model_dir} --all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Slices for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_spec = [\n",
    "  tfma.slicer.SingleSliceSpec(),\n",
    "  tfma.slicer.SingleSliceSpec(columns=['occupation'])\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run this on Dataflow by setting the `pipeline_options` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result = tfma.run_model_analysis(\n",
    "    eval_shared_model=tfma.default_eval_shared_model(\n",
    "        eval_saved_model_path=eval_model_dir,\n",
    "        example_weight_key=WEIGHT_COLUMN_NAME) , \n",
    "    data_location=DATA_FILES, \n",
    "    file_format='text', \n",
    "    slice_spec=slice_spec,  \n",
    "    output_path=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result.slicing_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visalise and analyze evalation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfma.view.render_slicing_metrics(\n",
    "    result=eval_result, \n",
    "    slicing_column='occupation'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
