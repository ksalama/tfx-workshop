{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Validation with TFMA\n",
    "\n",
    "In this lab, we use [TensorFlow Model Analysis](https://www.tensorflow.org/tfx/guide/tfma) to assess the quality of the trained model. This lab covers the following:\n",
    "1. **Export** evaluation saved model\n",
    "2. Define **data slices** for analysis\n",
    "3. Generat **evaluation** the metrics\n",
    "4. **Visualize** results\n",
    "5. **Bonus**: Use the **What-If** Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q apache-beam==2.16 pyarrow==0.14.0 tfx-bsl==0.15.1 tfx==0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow.io as tf_io\n",
    "import tensorflow_transform as tft\n",
    "import tensorflow_model_analysis as tfma\n",
    "import tensorflow_data_validation as tfdv\n",
    "from tensorflow_transform.tf_metadata import schema_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKSPACE = 'workspace' # you can set to a GCS location\n",
    "RAW_SCHEMA_LOCATION = os.path.join(WORKSPACE, 'raw_schema.pbtxt')\n",
    "DATA_DIR = os.path.join(WORKSPACE, 'raw_data')\n",
    "TRANSFORM_ARTEFACTS_DIR = os.path.join(WORKSPACE, 'transform_artifacts')\n",
    "DATA_FILES = os.path.join(DATA_DIR,'*.csv')\n",
    "MODELS_DIR = os.path.join(WORKSPACE, 'models')\n",
    "MODEL_NAME = 'dnn_classifier'\n",
    "MODEL_DIR = os.path.join(MODELS_DIR, MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load TFT Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_output = tft.TFTransformOutput(TRANSFORM_ARTEFACTS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Export Evaluation Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADER = ['age', 'workclass', 'fnlwgt', 'education', 'education_num',\n",
    "          'marital_status', 'occupation', 'relationship', 'race', 'gender',\n",
    "          'capital_gain', 'capital_loss', 'hours_per_week',\n",
    "          'native_country', 'income_bracket']\n",
    "\n",
    "HEADER_DEFAULTS = [[0], [''], [0], [''], [0], [''], [''], [''], [''], [''],\n",
    "                   [0], [0], [0], [''], ['']]\n",
    "\n",
    "TARGET_FEATURE_NAME = 'income_bracket'\n",
    "TARGET_LABELS = [' <=50K', ' >50K']\n",
    "WEIGHT_COLUMN_NAME = 'fnlwgt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Implement eval_input_receiver_fn\n",
    "This function expect **raw** data interface, then it applies the **transformation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_input_receiver_fn():\n",
    "    \n",
    "    receiver_tensors = {'examples': tf.placeholder(dtype=tf.string, shape=[None])}\n",
    "    columns = tf.decode_csv(receiver_tensors['examples'], record_defaults=HEADER_DEFAULTS)\n",
    "    \n",
    "    features = dict(zip(HEADER, columns))\n",
    "    \n",
    "    for feature_name in features:\n",
    "        if features[feature_name].dtype == tf.int32:\n",
    "            features[feature_name] = tf.cast(features[feature_name], tf.int64)\n",
    "        features[feature_name] = tf.reshape(features[feature_name], (-1, 1))\n",
    "        \n",
    "    transformed_features = transform_output.transform_raw_features(features)\n",
    "    features.update(transformed_features)\n",
    "\n",
    "    return tfma.export.EvalInputReceiver(\n",
    "        features=features,\n",
    "        receiver_tensors=receiver_tensors,\n",
    "        labels=features[TARGET_FEATURE_NAME]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Export an evaluation saved model\n",
    "First, we load the estimator..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "class Parameters: pass\n",
    "\n",
    "estimator_file_path = os.path.join(WORKSPACE, 'estimator.joblib')\n",
    "estimator = joblib.load(estimator_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_optimizer(initial_learning_rate, decay_steps):\n",
    "    learning_rate = tf.train.cosine_decay_restarts(\n",
    "        initial_learning_rate,\n",
    "        tf.train.get_global_step(),\n",
    "        first_decay_steps=50,\n",
    "        t_mul=2.0,\n",
    "        m_mul=1.0,\n",
    "        alpha=0.0,\n",
    "    )\n",
    "    \n",
    "    tf.summary.scalar('learning_rate', learning_rate)\n",
    "    return tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "def metric_fn(labels, predictions):\n",
    "    \n",
    "    metrics = {}\n",
    "    label_index = tf.contrib.lookup.index_table_from_tensor(tf.constant(TARGET_LABELS)).lookup(labels)\n",
    "    one_hot_labels = tf.one_hot(label_index, len(TARGET_LABELS))\n",
    "    \n",
    "    metrics['mirco_accuracy'] = tf.metrics.mean_per_class_accuracy(\n",
    "        labels=label_index,\n",
    "        predictions=predictions['class_ids'],\n",
    "        num_classes=2\n",
    "    )\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_FEATURE_NAME = 'income_bracket'\n",
    "TARGET_LABELS = [' <=50K', ' >50K']\n",
    "WEIGHT_COLUMN_NAME = 'fnlwgt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "eval_model_dir = os.path.join(MODEL_DIR, \"export/evaluate\")\n",
    "if tf_io.gfile.exists(eval_model_dir):\n",
    "    tf_io.gfile.rmtree(eval_model_dir)\n",
    "\n",
    "eval_model_dir = tfma.export.export_eval_savedmodel(\n",
    "        estimator=estimator,\n",
    "        export_dir_base=eval_model_dir,\n",
    "        eval_input_receiver_fn=eval_input_receiver_fn\n",
    ")\n",
    "\n",
    "eval_model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!saved_model_cli show --dir=${eval_model_dir} --all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Slices for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_spec = [\n",
    "  tfma.slicer.SingleSliceSpec(),\n",
    "  tfma.slicer.SingleSliceSpec(columns=['occupation'])\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run this on Dataflow by setting the `pipeline_options` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result = tfma.run_model_analysis(\n",
    "    eval_shared_model=tfma.default_eval_shared_model(\n",
    "        eval_saved_model_path=eval_model_dir,\n",
    "        example_weight_key=WEIGHT_COLUMN_NAME) , \n",
    "    data_location=DATA_FILES, \n",
    "    file_format='text', \n",
    "    slice_spec=slice_spec,  \n",
    "    output_path=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result.slicing_metrics[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visalise and analyze evalation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfma.view.render_slicing_metrics(\n",
    "    result=eval_result, \n",
    "    slicing_column='occupation'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Bonus: Using What-If Tool\n",
    "\n",
    "The [What-if Tool](https://pair-code.github.io/what-if-tool/) makes it easy to efficiently and intuitively explore up to two models' performance on a dataset. Investigate model performances for a range of features in your dataset, optimization strategies and even manipulations to individual datapoint values. All this and more, in a visual way that requires minimal code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Export a tf.Example serving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_serving_input_fn():\n",
    "\n",
    "    raw_schema = tfdv.load_schema_text(RAW_SCHEMA_LOCATION)\n",
    "    raw_feature_spec = schema_utils.schema_as_feature_spec(raw_schema).feature_spec\n",
    "    raw_feature_spec.pop(TARGET_FEATURE_NAME)\n",
    "    raw_feature_spec.pop(WEIGHT_COLUMN_NAME) \n",
    "    \n",
    "    example_bytestring = tf.placeholder(\n",
    "        shape=[None], dtype=tf.string)\n",
    "    \n",
    "    features = tf.parse_example(\n",
    "        example_bytestring, raw_feature_spec)\n",
    "    \n",
    "    transformed_features = transform_output.transform_raw_features(features)\n",
    "      \n",
    "    return tf.estimator.export.ServingInputReceiver(\n",
    "        transformed_features, {'example': example_bytestring})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "export_dir = os.path.join(MODEL_DIR, 'export/wit')\n",
    "        \n",
    "saved_model_location = estimator.export_savedmodel(\n",
    "    export_dir_base=export_dir,\n",
    "    serving_input_receiver_fn=example_serving_input_fn\n",
    ")\n",
    "\n",
    "print(saved_model_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Creatre a prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = tf.contrib.predictor.from_saved_model(\n",
    "    export_dir = saved_model_location,\n",
    "    signature_def_key=\"predict\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_fn(examples):\n",
    "    examples = [example.SerializeToString() for example in examples]\n",
    "    return predictor({'example': examples})['probabilities'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Prepare the data for the What-if Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = os.path.join(DATA_DIR,'eval.csv')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "SAMPLE = 1000\n",
    "\n",
    "data_frame = pd.read_csv(DATA_FILE, names=HEADER).sample(n=SAMPLE)\n",
    "condition = lambda val: val == ' >50K'\n",
    "data_frame[TARGET_FEATURE_NAME] = np.where(\n",
    "    condition(data_frame[TARGET_FEATURE_NAME]), 1, 0)\n",
    "\n",
    "def df_to_examples(df, columns):\n",
    "    examples = []\n",
    "    for index, row in df.iterrows():\n",
    "        example = tf.train.Example()\n",
    "        for col in columns:\n",
    "            if df[col].dtype is np.dtype(np.int64):\n",
    "                example.features.feature[col].int64_list.value.append(int(row[col]))\n",
    "            elif df[col].dtype is np.dtype(np.float64):\n",
    "                example.features.feature[col].float_list.value.append(row[col])\n",
    "            elif row[col] == row[col]:\n",
    "                example.features.feature[col].bytes_list.value.append(row[col].encode('utf-8'))\n",
    "        examples.append(example)\n",
    "    return examples\n",
    "\n",
    "examples = df_to_examples(data_frame, HEADER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Run the What-if Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q witwidget\n",
    "\n",
    "from witwidget.notebook.visualization import WitConfigBuilder\n",
    "from witwidget.notebook.visualization import WitWidget\n",
    "\n",
    "config_builder = WitConfigBuilder(examples) \\\n",
    "    .set_custom_predict_fn(prediction_fn) \\\n",
    "    .set_target_feature(TARGET_FEATURE_NAME) \\\n",
    "    .set_label_vocab(TARGET_LABELS)\n",
    "\n",
    "_ = WitWidget(config_builder, height=800)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
