{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing and Feature Engineering with TFT\n",
    "\n",
    "In this lab, we use [TensorFlow Transform](https://www.tensorflow.org/tfx/guide/tft) (TFT) to perform the following:\n",
    "\n",
    "1. **Implement** transformation logic in **preprocess_fn.\n",
    "2. **Implement** a Beam pipeline:\n",
    " 1. **Analyze** and **transform** training data.\n",
    " 2. **Transform** evaluation data.\n",
    "3. **Run** pipeline to produce the transformed **data** and transform **artifacts**.\n",
    "\n",
    "\n",
    "<br/>\n",
    "<img valign=\"middle\" src=\"imgs/tft.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q apache-beam==2.16 pyarrow==0.14.0 tfx-bsl==0.15.1 tfx==0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import apache_beam as beam\n",
    "import tensorflow as tf\n",
    "import tensorflow_data_validation as tfdv\n",
    "import tensorflow_transform as tft\n",
    "import tensorflow_transform.beam as tft_beam\n",
    "from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "from tensorflow_transform.tf_metadata import dataset_schema\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "print('TF version: {}'.format(tf.__version__))\n",
    "print('TFDV version: {}'.format(tfdv.__version__))\n",
    "print('TFT version: {}'.format(tft.__version__))\n",
    "print('Apache Beam version: {}'.format(beam.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKSPACE = 'workspace' # you can set to a GCS location\n",
    "DATA_DIR = os.path.join(WORKSPACE, 'raw_data')\n",
    "TRAIN_DATA_FILE = os.path.join(DATA_DIR,'train.csv')\n",
    "EVAL_DATA_FILE = os.path.join(DATA_DIR,'eval.csv')\n",
    "RAW_SCHEMA_LOCATION = os.path.join(WORKSPACE, 'raw_schema.pbtxt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Implement transformation logic\n",
    "We make use of the raw schema to perform metadata-driven feature handling, as follows:\n",
    "1. Scale numeric features with z-score\n",
    "2. Integerise categorical features\n",
    "\n",
    "Ather transformations can be performed, including bucketization, polynomial expantion, clipping, or custom formulas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADER = ['age', 'workclass', 'fnlwgt', 'education', 'education_num',\n",
    "          'marital_status', 'occupation', 'relationship', 'race', 'gender',\n",
    "          'capital_gain', 'capital_loss', 'hours_per_week',\n",
    "          'native_country', 'income_bracket']\n",
    "TARGET_FEATURE_NAME = 'income_bracket'\n",
    "WEIGHT_COLUMN_NAME = 'fnlwgt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_preprocessing_fn(raw_schema):\n",
    "        \n",
    "    def preprocessing_fn(input_features):\n",
    "        \n",
    "        processed_features = {}\n",
    "        \n",
    "        for feature in raw_schema.feature:\n",
    "            feature_name = feature.name\n",
    "\n",
    "            # Pass the target and weight features as is.\n",
    "            if feature_name in [TARGET_FEATURE_NAME, WEIGHT_COLUMN_NAME]:\n",
    "                processed_features[feature_name] = input_features[feature_name]\n",
    "                continue\n",
    "\n",
    "            if feature.type == 1:\n",
    "                # Extract vocabulary and integerize categorical features.\n",
    "                processed_features[feature_name + \"_integerized\"] = tft.compute_and_apply_vocabulary(\n",
    "                    input_features[feature_name], vocab_filename=feature_name)\n",
    "            else:\n",
    "                # normalize numeric features.\n",
    "                processed_features[feature_name + \"_scaled\"] = tft.scale_to_z_score(\n",
    "                    input_features[feature_name])\n",
    "\n",
    "        # Bucketize age using quantiles. \n",
    "        quantiles = tft.quantiles(input_features[\"age\"], num_buckets=5, epsilon=0.01)\n",
    "        processed_features[\"age_bucketized\"] = tft.apply_buckets(\n",
    "          input_features[\"age\"], bucket_boundaries=quantiles)\n",
    "        \n",
    "        # Feature creation\n",
    "        education_to_age_ratio = input_features[\"age\"] / input_features[\"education_num\"]\n",
    "        capital_indicator = input_features['capital_gain'] > input_features['capital_loss']\n",
    "        processed_features['education_to_age_ratio'] = tf.cast(education_to_age_ratio, tf.float32)\n",
    "        processed_features['capital_indicator'] =tf.cast(capital_indicator, tf.int64)\n",
    "    \n",
    "        return processed_features\n",
    "\n",
    "    return preprocessing_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implement a Beam pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(args):\n",
    "    \n",
    "    pipeline_options = beam.pipeline.PipelineOptions(flags=[], **args)\n",
    "    \n",
    "    raw_schema_location = args['raw_schema_location']\n",
    "    raw_train_data_location = args['raw_train_data_location']\n",
    "    raw_eval_data_location = args['raw_eval_data_location']\n",
    "    transformed_train_data_location = args['transformed_train_data_location']\n",
    "    transformed_eval_data_location = args['transformed_eval_data_location']\n",
    "    transform_artefact_location = args['transform_artefact_location']\n",
    "    temporary_dir = args['temporary_dir']\n",
    "    runner = args['runner']\n",
    "\n",
    "    # Load TFDV schema and create tft schema from it.\n",
    "    source_raw_schema = tfdv.load_schema_text(raw_schema_location)\n",
    "    raw_feature_spec = schema_utils.schema_as_feature_spec(source_raw_schema).feature_spec\n",
    "    \n",
    "    # Since the raw_feature_spec doesn't include the weight column, we need to add it. \n",
    "    raw_feature_spec[WEIGHT_COLUMN_NAME] = tf.FixedLenFeature(\n",
    "        shape=[1], dtype=tf.int64, default_value=None)\n",
    "    \n",
    "    raw_metadata = dataset_metadata.DatasetMetadata(\n",
    "      dataset_schema.from_feature_spec(raw_feature_spec))\n",
    "\n",
    "    with beam.Pipeline(runner, options=pipeline_options) as pipeline:\n",
    "        with tft_beam.Context(temporary_dir):\n",
    "            \n",
    "            converter = tft.coders.CsvCoder(column_names=HEADER, \n",
    "                schema=raw_metadata.schema)\n",
    "            \n",
    "            ###### analyze & transform trainining data ###############################\n",
    "\n",
    "            # Read raw training csv data.\n",
    "            step = 'Train'\n",
    "            \n",
    "            raw_train_data = (\n",
    "                pipeline\n",
    "                  | '{} - Read Raw Data'.format(step) >> beam.io.textio.ReadFromText(raw_train_data_location)\n",
    "                  | '{} - Remove Empty Rows'.format(step) >> beam.Filter(lambda line: line)\n",
    "                  | '{} - Decode CSV Data'.format(step) >> beam.Map(converter.decode)\n",
    "            )\n",
    "      \n",
    "            # Create a train dataset from the data and schema.\n",
    "            raw_train_dataset = (raw_train_data, raw_metadata)\n",
    "\n",
    "            # Analyze and transform raw_train_dataset to produced transformed_train_dataset and transform_fn.\n",
    "            transformed_train_dataset, transform_fn = (\n",
    "                raw_train_dataset \n",
    "                | '{} - Analyze & Transform'.format(step) >> tft_beam.AnalyzeAndTransformDataset(\n",
    "                      make_preprocessing_fn(source_raw_schema))\n",
    "            )\n",
    "  \n",
    "            # Get data and schema separately from the transformed_train_dataset.\n",
    "            transformed_train_data, transformed_metadata = transformed_train_dataset\n",
    "\n",
    "            # write transformed train data to sink.\n",
    "            _ = (\n",
    "                transformed_train_data \n",
    "                | '{} - Write Transformed Data'.format(step) >> beam.io.tfrecordio.WriteToTFRecord(\n",
    "                    file_path_prefix=transformed_train_data_location,\n",
    "                    file_name_suffix=\".tfrecords\",\n",
    "                    coder=tft.coders.ExampleProtoCoder(transformed_metadata.schema))\n",
    "            )\n",
    "\n",
    "            ###### transform evaluation data #########################################\n",
    "\n",
    "            # Read raw training csv data.\n",
    "            step = 'Eval'\n",
    "\n",
    "            raw_eval_data = (\n",
    "            pipeline\n",
    "              | '{} - Read Raw Data'.format(step) >> beam.io.textio.ReadFromText(raw_eval_data_location)\n",
    "              | '{} - Remove Empty Rows'.format(step) >> beam.Filter(lambda line: line)\n",
    "              | '{} - Decode CSV Data'.format(step) >> beam.Map(converter.decode)\n",
    "            )\n",
    "      \n",
    "            # Create a eval dataset from the data and schema.\n",
    "            raw_eval_dataset = (raw_eval_data, raw_metadata)\n",
    "\n",
    "            # Transform eval data based on produced transform_fn.\n",
    "            transformed_eval_dataset = (\n",
    "                (raw_eval_dataset, transform_fn) \n",
    "                | '{} - Transform'.format(step) >> tft_beam.TransformDataset()\n",
    "            )\n",
    "\n",
    "            # Get data from the transformed_eval_dataset.\n",
    "            transformed_eval_data, _ = transformed_eval_dataset\n",
    "\n",
    "            # Write transformed eval data to sink.\n",
    "            _ = (\n",
    "                transformed_eval_data \n",
    "                | '{} - Write Transformed Data'.format(step) >> beam.io.tfrecordio.WriteToTFRecord(\n",
    "                    file_path_prefix=transformed_eval_data_location,\n",
    "                    file_name_suffix=\".tfrecords\",\n",
    "                    coder=tft.coders.ExampleProtoCoder(transformed_metadata.schema))\n",
    "            )\n",
    "\n",
    "            ###### write transformation metadata #######################################################\n",
    "\n",
    "            # Write transform_fn.\n",
    "            _ = (\n",
    "              transform_fn \n",
    "              | 'Write Transform Artefacts' >> tft_beam.WriteTransformFn(\n",
    "                  transform_artefact_location)\n",
    "            )\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Data Transformation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSFORM_ARTEFACTS_DIR = os.path.join(WORKSPACE,'transform_artifacts')\n",
    "TRANSFORMED_DATA_DIR = os.path.join(WORKSPACE,'transformed_data')\n",
    "TEMP_DIR = os.path.join(WORKSPACE, 'tmp')\n",
    "\n",
    "runner = 'DirectRunner'\n",
    "\n",
    "args = {\n",
    "    \n",
    "    'runner': runner,\n",
    "\n",
    "    'raw_schema_location': RAW_SCHEMA_LOCATION,\n",
    "\n",
    "    'raw_train_data_location': TRAIN_DATA_FILE,\n",
    "    'raw_eval_data_location': EVAL_DATA_FILE,\n",
    "\n",
    "    'transformed_train_data_location':  os.path.join(TRANSFORMED_DATA_DIR, \"train\"),\n",
    "    'transformed_eval_data_location':  os.path.join(TRANSFORMED_DATA_DIR, \"eval\"),\n",
    "    'transform_artefact_location':  TRANSFORM_ARTEFACTS_DIR,\n",
    "    \n",
    "    'temporary_dir': TEMP_DIR\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.io import gfile\n",
    "\n",
    "if gfile.exists(TEMP_DIR):\n",
    "    print(\"Removing {} contents...\".format(TEMP_DIR))\n",
    "    gfile.rmtree(TRANSFORMED_DATA_DIR)\n",
    "    \n",
    "if gfile.exists(TRANSFORMED_DATA_DIR):\n",
    "    print(\"Removing {} contents...\".format(TRANSFORMED_DATA_DIR))\n",
    "    gfile.rmtree(TRANSFORMED_DATA_DIR)\n",
    "          \n",
    "if gfile.exists(TRANSFORM_ARTEFACTS_DIR):\n",
    "    print(\"Removing {} contents...\".format(TRANSFORM_ARTEFACTS_DIR))\n",
    "    gfile.rmtree(TRANSFORM_ARTEFACTS_DIR)\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "print(\"Running TF Transform pipeline...\")\n",
    "print(\"\")\n",
    "%time run_pipeline(args)\n",
    "print(\"\")\n",
    "print(\"Pipeline is done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check TFT outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {TRANSFORM_ARTEFACTS_DIR}/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tft_output = tft.TFTransformOutput(TRANSFORM_ARTEFACTS_DIR)\n",
    "transform_feature_spec = tft_output.transformed_feature_spec()\n",
    "transform_feature_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_example(example):\n",
    "  # Parse the input `tf.Example` proto using the dictionary above.\n",
    "  return tf.io.parse_single_example(example, transform_feature_spec)\n",
    "\n",
    "dataset = tf.data.TFRecordDataset(TRANSFORMED_DATA_DIR + \"/train-00000-of-00001.tfrecords\")\n",
    "for record in dataset.take(3).map(_parse_example):\n",
    "    print(record)\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Step: TF Estimator\n",
    "Model Training and Evaluation with TensorFlow Estimator API."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
