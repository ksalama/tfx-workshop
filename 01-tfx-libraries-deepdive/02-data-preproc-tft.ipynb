{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing and Feature Engineering with TFT\n",
    "\n",
    "In this lab, we use [TensorFlow Transform](https://www.tensorflow.org/tfx/guide/tft) (TFT) to perform the following:\n",
    "\n",
    "1. **Implement** transformation logic in **preprocess_fn.\n",
    "2. **Implement** a Beam pipeline:\n",
    " 1. **Analyze** and **transform** training data.\n",
    " 2. **Transform** evaluation data.\n",
    "3. **Run** pipeline to produce the transformed **data** and transform **artifacts**.\n",
    "\n",
    "\n",
    "<br/>\n",
    "<img valign=\"middle\" src=\"imgs/tft.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q tensorflow_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version: 1.15.0\n",
      "TFDV version: 0.15.0\n",
      "TFT version: 0.15.0\n",
      "Apache Beam version: 2.16.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import apache_beam as beam\n",
    "import tensorflow as tf\n",
    "import tensorflow_data_validation as tfdv\n",
    "import tensorflow_transform as tft\n",
    "import tensorflow_transform.beam as tft_beam\n",
    "from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "from tensorflow_transform.tf_metadata import dataset_schema\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "print('TF version: {}'.format(tf.__version__))\n",
    "print('TFDV version: {}'.format(tfdv.__version__))\n",
    "print('TFT version: {}'.format(tft.__version__))\n",
    "print('Apache Beam version: {}'.format(beam.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKSPACE = 'workspace' # you can set to a GCS location\n",
    "DATA_DIR = os.path.join(WORKSPACE, 'raw_data')\n",
    "TRAIN_DATA_FILE = os.path.join(DATA_DIR,'train.csv')\n",
    "EVAL_DATA_FILE = os.path.join(DATA_DIR,'eval.csv')\n",
    "RAW_SCHEMA_LOCATION = os.path.join(WORKSPACE, 'raw_schema.pbtxt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Implement transformation logic\n",
    "We make use of the raw schema to perform metadata-driven feature handling, as follows:\n",
    "1. Scale numeric features with z-score\n",
    "2. Integerise categorical features\n",
    "\n",
    "Ather transformations can be performed, including bucketization, polynomial expantion, clipping, or custom formulas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADER = ['age', 'workclass', 'fnlwgt', 'education', 'education_num',\n",
    "          'marital_status', 'occupation', 'relationship', 'race', 'gender',\n",
    "          'capital_gain', 'capital_loss', 'hours_per_week',\n",
    "          'native_country', 'income_bracket']\n",
    "TARGET_FEATURE_NAME = 'income_bracket'\n",
    "WEIGHT_COLUMN_NAME = 'fnlwgt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_preprocessing_fn(raw_schema):\n",
    "        \n",
    "    def preprocessing_fn(input_features):\n",
    "        \n",
    "        processed_features = {}\n",
    "        \n",
    "        for feature in raw_schema.feature:\n",
    "            feature_name = feature.name\n",
    "\n",
    "            # Pass the target and weight features as is.\n",
    "            if feature_name in [TARGET_FEATURE_NAME, WEIGHT_COLUMN_NAME]:\n",
    "                processed_features[feature_name] = input_features[feature_name]\n",
    "                continue\n",
    "\n",
    "            if feature.type == 1:\n",
    "                # Extract vocabulary and integerize categorical features.\n",
    "                processed_features[feature_name + \"_integerized\"] = tft.compute_and_apply_vocabulary(\n",
    "                    input_features[feature_name], vocab_filename=feature_name)\n",
    "            else:\n",
    "                # normalize numeric features.\n",
    "                processed_features[feature_name + \"_scaled\"] = tft.scale_to_z_score(\n",
    "                    input_features[feature_name])\n",
    "\n",
    "        # Bucketize age using quantiles. \n",
    "        quantiles = tft.quantiles(input_features[\"age\"], num_buckets=5, epsilon=0.01)\n",
    "        processed_features[\"age_bucketized\"] = tft.apply_buckets(\n",
    "          input_features[\"age\"], bucket_boundaries=quantiles)\n",
    "        \n",
    "        # Feature creation\n",
    "        education_to_age_ratio = input_features[\"age\"] / input_features[\"education_num\"]\n",
    "        capital_indicator = input_features['capital_gain'] > input_features['capital_loss']\n",
    "        processed_features['education_to_age_ratio'] = tf.cast(education_to_age_ratio, tf.float32)\n",
    "        processed_features['capital_indicator'] =tf.cast(capital_indicator, tf.int64)\n",
    "    \n",
    "        return processed_features\n",
    "\n",
    "    return preprocessing_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implement a Beam pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(args):\n",
    "    \n",
    "    pipeline_options = beam.pipeline.PipelineOptions(flags=[], **args)\n",
    "    \n",
    "    raw_schema_location = args['raw_schema_location']\n",
    "    raw_train_data_location = args['raw_train_data_location']\n",
    "    raw_eval_data_location = args['raw_eval_data_location']\n",
    "    transformed_train_data_location = args['transformed_train_data_location']\n",
    "    transformed_eval_data_location = args['transformed_eval_data_location']\n",
    "    transform_artefact_location = args['transform_artefact_location']\n",
    "    temporary_dir = args['temporary_dir']\n",
    "    runner = args['runner']\n",
    "\n",
    "    # Load TFDV schema and create tft schema from it.\n",
    "    source_raw_schema = tfdv.load_schema_text(raw_schema_location)\n",
    "    raw_feature_spec = schema_utils.schema_as_feature_spec(source_raw_schema).feature_spec\n",
    "    \n",
    "    # Since the raw_feature_spec doesn't include the weight column, we need to add it. \n",
    "    raw_feature_spec[WEIGHT_COLUMN_NAME] = tf.FixedLenFeature(\n",
    "        shape=[1], dtype=tf.int64, default_value=None)\n",
    "    \n",
    "    raw_metadata = dataset_metadata.DatasetMetadata(\n",
    "      dataset_schema.from_feature_spec(raw_feature_spec))\n",
    "\n",
    "    with beam.Pipeline(runner, options=pipeline_options) as pipeline:\n",
    "        with tft_beam.Context(temporary_dir):\n",
    "            \n",
    "            converter = tft.coders.CsvCoder(column_names=HEADER, \n",
    "                schema=raw_metadata.schema)\n",
    "            \n",
    "            ###### analyze & transform trainining data ###############################\n",
    "\n",
    "            # Read raw training csv data.\n",
    "            step = 'Train'\n",
    "            \n",
    "            raw_train_data = (\n",
    "                pipeline\n",
    "                  | '{} - Read Raw Data'.format(step) >> beam.io.textio.ReadFromText(raw_train_data_location)\n",
    "                  | '{} - Remove Empty Rows'.format(step) >> beam.Filter(lambda line: line)\n",
    "                  | '{} - Decode CSV Data'.format(step) >> beam.Map(converter.decode)\n",
    "            )\n",
    "      \n",
    "            # Create a train dataset from the data and schema.\n",
    "            raw_train_dataset = (raw_train_data, raw_metadata)\n",
    "\n",
    "            # Analyze and transform raw_train_dataset to produced transformed_train_dataset and transform_fn.\n",
    "            transformed_train_dataset, transform_fn = (\n",
    "                raw_train_dataset \n",
    "                | '{} - Analyze & Transform'.format(step) >> tft_beam.AnalyzeAndTransformDataset(\n",
    "                      make_preprocessing_fn(source_raw_schema))\n",
    "            )\n",
    "  \n",
    "            # Get data and schema separately from the transformed_train_dataset.\n",
    "            transformed_train_data, transformed_metadata = transformed_train_dataset\n",
    "\n",
    "            # write transformed train data to sink.\n",
    "            _ = (\n",
    "                transformed_train_data \n",
    "                | '{} - Write Transformed Data'.format(step) >> beam.io.tfrecordio.WriteToTFRecord(\n",
    "                    file_path_prefix=transformed_train_data_location,\n",
    "                    file_name_suffix=\".tfrecords\",\n",
    "                    coder=tft.coders.ExampleProtoCoder(transformed_metadata.schema))\n",
    "            )\n",
    "\n",
    "            ###### transform evaluation data #########################################\n",
    "\n",
    "            # Read raw training csv data.\n",
    "            step = 'Eval'\n",
    "\n",
    "            raw_eval_data = (\n",
    "            pipeline\n",
    "              | '{} - Read Raw Data'.format(step) >> beam.io.textio.ReadFromText(raw_eval_data_location)\n",
    "              | '{} - Remove Empty Rows'.format(step) >> beam.Filter(lambda line: line)\n",
    "              | '{} - Decode CSV Data'.format(step) >> beam.Map(converter.decode)\n",
    "            )\n",
    "      \n",
    "            # Create a eval dataset from the data and schema.\n",
    "            raw_eval_dataset = (raw_eval_data, raw_metadata)\n",
    "\n",
    "            # Transform eval data based on produced transform_fn.\n",
    "            transformed_eval_dataset = (\n",
    "                (raw_eval_dataset, transform_fn) \n",
    "                | '{} - Transform'.format(step) >> tft_beam.TransformDataset()\n",
    "            )\n",
    "\n",
    "            # Get data from the transformed_eval_dataset.\n",
    "            transformed_eval_data, _ = transformed_eval_dataset\n",
    "\n",
    "            # Write transformed eval data to sink.\n",
    "            _ = (\n",
    "                transformed_eval_data \n",
    "                | '{} - Write Transformed Data'.format(step) >> beam.io.tfrecordio.WriteToTFRecord(\n",
    "                    file_path_prefix=transformed_eval_data_location,\n",
    "                    file_name_suffix=\".tfrecords\",\n",
    "                    coder=tft.coders.ExampleProtoCoder(transformed_metadata.schema))\n",
    "            )\n",
    "\n",
    "            ###### write transformation metadata #######################################################\n",
    "\n",
    "            # Write transform_fn.\n",
    "            _ = (\n",
    "              transform_fn \n",
    "              | 'Write Transform Artefacts' >> tft_beam.WriteTransformFn(\n",
    "                  transform_artefact_location)\n",
    "            )\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Data Transformation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSFORM_ARTEFACTS_DIR = os.path.join(WORKSPACE,'transform_artifacts')\n",
    "TRANSFORMED_DATA_DIR = os.path.join(WORKSPACE,'transformed_data')\n",
    "TEMP_DIR = os.path.join(WORKSPACE, 'tmp')\n",
    "\n",
    "runner = 'DirectRunner'\n",
    "\n",
    "args = {\n",
    "    \n",
    "    'runner': runner,\n",
    "\n",
    "    'raw_schema_location': RAW_SCHEMA_LOCATION,\n",
    "\n",
    "    'raw_train_data_location': TRAIN_DATA_FILE,\n",
    "    'raw_eval_data_location': EVAL_DATA_FILE,\n",
    "\n",
    "    'transformed_train_data_location':  os.path.join(TRANSFORMED_DATA_DIR, \"train\"),\n",
    "    'transformed_eval_data_location':  os.path.join(TRANSFORMED_DATA_DIR, \"eval\"),\n",
    "    'transform_artefact_location':  TRANSFORM_ARTEFACTS_DIR,\n",
    "    \n",
    "    'temporary_dir': TEMP_DIR\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing workspace/tmp contents...\n",
      "Removing workspace/transform_artifacts contents...\n",
      "Running TF Transform pipeline...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Deleting 1 existing files in target path matching: \n",
      "WARNING:root:Deleting 1 existing files in target path matching: \n",
      "WARNING:root:Deleting 1 existing files in target path matching: \n",
      "WARNING:root:Deleting 1 existing files in target path matching: \n",
      "WARNING:root:Deleting 1 existing files in target path matching: \n",
      "WARNING:root:Deleting 1 existing files in target path matching: \n",
      "WARNING:root:Deleting 1 existing files in target path matching: \n",
      "WARNING:root:Deleting 1 existing files in target path matching: \n",
      "WARNING:root:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 4s, sys: 1.2 s, total: 1min 5s\n",
      "Wall time: 1min 6s\n",
      "\n",
      "Pipeline is done.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.io import gfile\n",
    "\n",
    "if gfile.exists(TEMP_DIR):\n",
    "    print(\"Removing {} contents...\".format(TEMP_DIR))\n",
    "    gfile.rmtree(TRANSFORMED_DATA_DIR)\n",
    "    \n",
    "if gfile.exists(TRANSFORMED_DATA_DIR):\n",
    "    print(\"Removing {} contents...\".format(TRANSFORMED_DATA_DIR))\n",
    "    gfile.rmtree(TRANSFORMED_DATA_DIR)\n",
    "          \n",
    "if gfile.exists(TRANSFORM_ARTEFACTS_DIR):\n",
    "    print(\"Removing {} contents...\".format(TRANSFORM_ARTEFACTS_DIR))\n",
    "    gfile.rmtree(TRANSFORM_ARTEFACTS_DIR)\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "print(\"Running TF Transform pipeline...\")\n",
    "print(\"\")\n",
    "%time run_pipeline(args)\n",
    "print(\"\")\n",
    "print(\"Pipeline is done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check TFT outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workspace/transform_artifacts/transform_fn:\n",
      "assets\tsaved_model.pb\tvariables\n",
      "\n",
      "workspace/transform_artifacts/transformed_metadata:\n",
      "schema.pbtxt\n"
     ]
    }
   ],
   "source": [
    "!ls {TRANSFORM_ARTEFACTS_DIR}/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'age_bucketized': FixedLenFeature(shape=[1], dtype=tf.int64, default_value=None),\n",
       " 'age_scaled': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
       " 'capital_gain_scaled': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
       " 'capital_indicator': FixedLenFeature(shape=[1], dtype=tf.int64, default_value=None),\n",
       " 'capital_loss_scaled': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
       " 'education_integerized': FixedLenFeature(shape=[1], dtype=tf.int64, default_value=None),\n",
       " 'education_num_scaled': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
       " 'education_to_age_ratio': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
       " 'fnlwgt': FixedLenFeature(shape=[1], dtype=tf.int64, default_value=None),\n",
       " 'gender_integerized': FixedLenFeature(shape=[1], dtype=tf.int64, default_value=None),\n",
       " 'hours_per_week_scaled': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None),\n",
       " 'income_bracket': FixedLenFeature(shape=[1], dtype=tf.string, default_value=None),\n",
       " 'marital_status_integerized': FixedLenFeature(shape=[1], dtype=tf.int64, default_value=None),\n",
       " 'native_country_integerized': FixedLenFeature(shape=[1], dtype=tf.int64, default_value=None),\n",
       " 'occupation_integerized': FixedLenFeature(shape=[1], dtype=tf.int64, default_value=None),\n",
       " 'race_integerized': FixedLenFeature(shape=[1], dtype=tf.int64, default_value=None),\n",
       " 'relationship_integerized': FixedLenFeature(shape=[1], dtype=tf.int64, default_value=None),\n",
       " 'workclass_integerized': FixedLenFeature(shape=[1], dtype=tf.int64, default_value=None)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tft_output = tft.TFTransformOutput(TRANSFORM_ARTEFACTS_DIR)\n",
    "transform_feature_spec = tft_output.transformed_feature_spec()\n",
    "transform_feature_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'income_bracket': <tf.Tensor: id=2380, shape=(1,), dtype=string, numpy=array([b' <=50K'], dtype=object)>, 'capital_gain_scaled': <tf.Tensor: id=2371, shape=(1,), dtype=float32, numpy=array([0.14845294], dtype=float32)>, 'native_country_integerized': <tf.Tensor: id=2382, shape=(1,), dtype=int64, numpy=array([0])>, 'capital_loss_scaled': <tf.Tensor: id=2373, shape=(1,), dtype=float32, numpy=array([-0.21665956], dtype=float32)>, 'relationship_integerized': <tf.Tensor: id=2385, shape=(1,), dtype=int64, numpy=array([1])>, 'fnlwgt': <tf.Tensor: id=2377, shape=(1,), dtype=int64, numpy=array([77516])>, 'occupation_integerized': <tf.Tensor: id=2383, shape=(1,), dtype=int64, numpy=array([3])>, 'education_num_scaled': <tf.Tensor: id=2375, shape=(1,), dtype=float32, numpy=array([1.1347382], dtype=float32)>, 'age_bucketized': <tf.Tensor: id=2369, shape=(1,), dtype=int64, numpy=array([2])>, 'age_scaled': <tf.Tensor: id=2370, shape=(1,), dtype=float32, numpy=array([0.03067062], dtype=float32)>, 'hours_per_week_scaled': <tf.Tensor: id=2379, shape=(1,), dtype=float32, numpy=array([-0.03542928], dtype=float32)>, 'workclass_integerized': <tf.Tensor: id=2386, shape=(1,), dtype=int64, numpy=array([4])>, 'education_integerized': <tf.Tensor: id=2374, shape=(1,), dtype=int64, numpy=array([2])>, 'race_integerized': <tf.Tensor: id=2384, shape=(1,), dtype=int64, numpy=array([0])>, 'marital_status_integerized': <tf.Tensor: id=2381, shape=(1,), dtype=int64, numpy=array([1])>, 'education_to_age_ratio': <tf.Tensor: id=2376, shape=(1,), dtype=float32, numpy=array([3.], dtype=float32)>, 'gender_integerized': <tf.Tensor: id=2378, shape=(1,), dtype=int64, numpy=array([0])>, 'capital_indicator': <tf.Tensor: id=2372, shape=(1,), dtype=int64, numpy=array([1])>}\n",
      "\n",
      "{'income_bracket': <tf.Tensor: id=2398, shape=(1,), dtype=string, numpy=array([b' <=50K'], dtype=object)>, 'capital_gain_scaled': <tf.Tensor: id=2389, shape=(1,), dtype=float32, numpy=array([-0.14592052], dtype=float32)>, 'native_country_integerized': <tf.Tensor: id=2400, shape=(1,), dtype=int64, numpy=array([0])>, 'capital_loss_scaled': <tf.Tensor: id=2391, shape=(1,), dtype=float32, numpy=array([-0.21665956], dtype=float32)>, 'relationship_integerized': <tf.Tensor: id=2403, shape=(1,), dtype=int64, numpy=array([0])>, 'fnlwgt': <tf.Tensor: id=2395, shape=(1,), dtype=int64, numpy=array([83311])>, 'occupation_integerized': <tf.Tensor: id=2401, shape=(1,), dtype=int64, numpy=array([2])>, 'education_num_scaled': <tf.Tensor: id=2393, shape=(1,), dtype=float32, numpy=array([1.1347382], dtype=float32)>, 'age_bucketized': <tf.Tensor: id=2387, shape=(1,), dtype=int64, numpy=array([4])>, 'age_scaled': <tf.Tensor: id=2388, shape=(1,), dtype=float32, numpy=array([0.8371091], dtype=float32)>, 'hours_per_week_scaled': <tf.Tensor: id=2397, shape=(1,), dtype=float32, numpy=array([-2.2221508], dtype=float32)>, 'workclass_integerized': <tf.Tensor: id=2404, shape=(1,), dtype=int64, numpy=array([1])>, 'education_integerized': <tf.Tensor: id=2392, shape=(1,), dtype=int64, numpy=array([2])>, 'race_integerized': <tf.Tensor: id=2402, shape=(1,), dtype=int64, numpy=array([0])>, 'marital_status_integerized': <tf.Tensor: id=2399, shape=(1,), dtype=int64, numpy=array([0])>, 'education_to_age_ratio': <tf.Tensor: id=2394, shape=(1,), dtype=float32, numpy=array([3.8461537], dtype=float32)>, 'gender_integerized': <tf.Tensor: id=2396, shape=(1,), dtype=int64, numpy=array([0])>, 'capital_indicator': <tf.Tensor: id=2390, shape=(1,), dtype=int64, numpy=array([0])>}\n",
      "\n",
      "{'income_bracket': <tf.Tensor: id=2416, shape=(1,), dtype=string, numpy=array([b' <=50K'], dtype=object)>, 'capital_gain_scaled': <tf.Tensor: id=2407, shape=(1,), dtype=float32, numpy=array([-0.14592052], dtype=float32)>, 'native_country_integerized': <tf.Tensor: id=2418, shape=(1,), dtype=int64, numpy=array([0])>, 'capital_loss_scaled': <tf.Tensor: id=2409, shape=(1,), dtype=float32, numpy=array([-0.21665956], dtype=float32)>, 'relationship_integerized': <tf.Tensor: id=2421, shape=(1,), dtype=int64, numpy=array([1])>, 'fnlwgt': <tf.Tensor: id=2413, shape=(1,), dtype=int64, numpy=array([215646])>, 'occupation_integerized': <tf.Tensor: id=2419, shape=(1,), dtype=int64, numpy=array([9])>, 'education_num_scaled': <tf.Tensor: id=2411, shape=(1,), dtype=float32, numpy=array([-0.4200592], dtype=float32)>, 'age_bucketized': <tf.Tensor: id=2405, shape=(1,), dtype=int64, numpy=array([2])>, 'age_scaled': <tf.Tensor: id=2406, shape=(1,), dtype=float32, numpy=array([-0.04264197], dtype=float32)>, 'hours_per_week_scaled': <tf.Tensor: id=2415, shape=(1,), dtype=float32, numpy=array([-0.03542928], dtype=float32)>, 'workclass_integerized': <tf.Tensor: id=2422, shape=(1,), dtype=int64, numpy=array([0])>, 'education_integerized': <tf.Tensor: id=2410, shape=(1,), dtype=int64, numpy=array([0])>, 'race_integerized': <tf.Tensor: id=2420, shape=(1,), dtype=int64, numpy=array([0])>, 'marital_status_integerized': <tf.Tensor: id=2417, shape=(1,), dtype=int64, numpy=array([2])>, 'education_to_age_ratio': <tf.Tensor: id=2412, shape=(1,), dtype=float32, numpy=array([4.2222223], dtype=float32)>, 'gender_integerized': <tf.Tensor: id=2414, shape=(1,), dtype=int64, numpy=array([0])>, 'capital_indicator': <tf.Tensor: id=2408, shape=(1,), dtype=int64, numpy=array([0])>}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def _parse_example(example):\n",
    "  # Parse the input `tf.Example` proto using the dictionary above.\n",
    "  return tf.io.parse_single_example(example, transform_feature_spec)\n",
    "\n",
    "dataset = tf.data.TFRecordDataset(TRANSFORMED_DATA_DIR + \"/train-00000-of-00001.tfrecords\")\n",
    "for record in dataset.take(3).map(_parse_example):\n",
    "    print(record)\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
