{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation with Estimator API\n",
    "In this lan, we use the [TensorFlow Estimator API](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator) to build, train, and evaluate a [DNNLinearCombinedClassifier](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNLinearCombinedClassifier). This lab covers the following:\n",
    "1. Implement a data **input_fn** using **transform schema**\n",
    "2. Create **feature columns** using **transform schema**\n",
    "3. Instantiate a premade **DNNCombinedClassifier**\n",
    "4. **Train** and **evaluate** the model.\n",
    "5. **Export** the model for **serving**.\n",
    "\n",
    "<br/>\n",
    "<img valign=\"middle\" src=\"imgs/tf-estimator.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow.io as tf_io\n",
    "import tensorflow_data_validation as tfdv\n",
    "import tensorflow_transform as tft\n",
    "\n",
    "print('TF version: {}'.format(tf.__version__))\n",
    "print('TFDV version: {}'.format(tfdv.__version__))\n",
    "print('TFT version: {}'.format(tft.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKSPACE = 'workspace' # you can set to a GCS location\n",
    "TRANSFORMED_DATA_DIR = os.path.join(WORKSPACE, 'transformed_data')\n",
    "TRANSFORM_ARTEFACTS_DIR = os.path.join(WORKSPACE, 'transform_artifacts')\n",
    "TRANSFORMED_TRAIN_DATA_FILE = os.path.join(TRANSFORMED_DATA_DIR,'train-*.tfrecords')\n",
    "TRANSFORMED_EVAL_DATA_FILE = os.path.join(TRANSFORMED_DATA_DIR,'eval-*.tfrecords')\n",
    "RAW_SCHEMA_LOCATION = os.path.join(WORKSPACE, 'raw_schema.pbtxt')\n",
    "MODELS_DIR = os.path.join(WORKSPACE, 'models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load TFT Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_output = tft.TFTransformOutput(TRANSFORM_ARTEFACTS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Implement TFRecords Input function\n",
    "* Use [tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) APIs: **list_files()**, **skip()**, **map()**, **filter()**, **batch()**, **shuffle()**, **repeat()**, **prefetch()**, **cache()**, etc.\n",
    "* Use [tf.data.experimental.make_csv_dataset](https://www.tensorflow.org/api_docs/python/tf/data/experimental/make_csv_dataset) to read and parse CSV data files.\n",
    "* Use [tf.data.experimental.make_batched_features_dataset](https://www.tensorflow.org/api_docs/python/tf/data/experimental/make_batched_features_dataset) to read and parse TFRecords data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_input_fn(tfrecords_files, \n",
    "                  batch_size, num_epochs=1, shuffle=False):\n",
    "   \n",
    "    def input_fn():\n",
    "        dataset = tf.data.experimental.make_batched_features_dataset(\n",
    "            file_pattern=tfrecords_files,\n",
    "            batch_size=batch_size,\n",
    "            features=transform_output.transformed_feature_spec(),\n",
    "            label_key=TARGET_FEATURE_NAME,\n",
    "            reader=tf.data.TFRecordDataset,\n",
    "            num_epochs=num_epochs,\n",
    "            shuffle=shuffle\n",
    "        )\n",
    "        return dataset\n",
    "\n",
    "    return input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Feature Columns\n",
    "\n",
    "<br/>\n",
    "<img valign=\"middle\" src=\"imgs/feature-columns.png\" width=\"800\">\n",
    "\n",
    "Base feature columns\n",
    "  1. [numeric_column](https://www.tensorflow.org/api_docs/python/tf/feature_column/numeric_column|)\n",
    "  2. [categorical_column_with_vocabulary_list](https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_list)\n",
    "  3. [categorical_column_with_vocabulary_file](https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file)\n",
    "  4. [categorical_column_with_identity](https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_identity)\n",
    "  5. [categorical_column_with_hash_buckets](https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_hash_bucket)\n",
    "\n",
    "Extended feature columns\n",
    "  1. [bucketized_column](https://www.tensorflow.org/api_docs/python/tf/feature_column/bucketized_column)\n",
    "  2. [indicator_column](https://www.tensorflow.org/api_docs/python/tf/feature_column/indicator_column)\n",
    "  3. [crossing_column](https://www.tensorflow.org/api_docs/python/tf/feature_column/crossed_column)\n",
    "  4. [embedding_column](https://www.tensorflow.org/api_docs/python/tf/feature_column/embedding_column)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_FEATURE_NAME = 'income_bracket'\n",
    "TARGET_LABELS = [' <=50K', ' >50K']\n",
    "WEIGHT_COLUMN_NAME = 'fnlwgt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating featuer columns can be **meta-data** driven, with the help of the **stransform schema**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def create_feature_columns():\n",
    "    \n",
    "    wide_columns = []\n",
    "    deep_columns = []\n",
    "    \n",
    "    transformed_features = transform_output.transformed_metadata.schema.feature\n",
    "\n",
    "    for feature in transformed_features:\n",
    "        if feature.name in [TARGET_FEATURE_NAME, WEIGHT_COLUMN_NAME]:\n",
    "            continue\n",
    "\n",
    "        # Categorical features\n",
    "        if hasattr(feature, 'int_domain') and feature.int_domain.is_categorical:\n",
    "            vocab_size = feature.int_domain.max + 1\n",
    "            \n",
    "            # Create a categotical feature column with identity\n",
    "            categorical_feature_column = tf.feature_column.categorical_column_with_identity(\n",
    "                feature.name, num_buckets=vocab_size\n",
    "            )\n",
    "            \n",
    "            wide_columns.append(categorical_feature_column)\n",
    "            \n",
    "            \n",
    "            # Create embedding column\n",
    "            embedding_feature_column = tf.feature_column.embedding_column(\n",
    "                categorical_feature_column, \n",
    "                dimension = int(math.sqrt(vocab_size))\n",
    "            )\n",
    "            \n",
    "            deep_columns.append(embedding_feature_column)\n",
    "            \n",
    "        \n",
    "        # Numeric features\n",
    "        else:\n",
    "            deep_columns.append(\n",
    "                tf.feature_column.numeric_column(feature.name))\n",
    "            \n",
    "    # Create crossing feature\n",
    "    education_X_occupation = tf.feature_column.crossed_column(\n",
    "        ['education_integerized', 'workclass_integerized'], hash_bucket_size=int(1e4))\n",
    "    wide_columns.append(education_X_occupation)\n",
    "\n",
    "        # Create embeddings for crossing column.\n",
    "    education_X_occupation_embedded = tf.feature_column.embedding_column(\n",
    "        education_X_occupation, dimension=10)\n",
    "    deep_columns.append(education_X_occupation_embedded)\n",
    "    \n",
    "    return wide_columns, deep_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wide, deep = create_feature_columns()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Instantiate an [Wide and Deep Estimator](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNLinearCombinedClassifier)\n",
    "1. An ML model that combines **generalization** (deep part) and **memorization** (wide part).\n",
    "2. **Categorical** (sparse) features are feed into the **wide** part, while **numerical** (dense) features are feed into **deep** part.\n",
    "3. You can make use of different representation of the same feature in both parts:\n",
    "    1. **Categorical** features can be **embedded**, and feed into **deep** part.\n",
    "    2. **Numerical** features can be **bucketized**, and feed into **wide** part.\n",
    "    \n",
    "See [Wide & Deep Learning: Better Together with TensorFlow](https://ai.googleblog.com/2016/06/wide-deep-learning-better-together-with.html) blog post for more details.\n",
    "\n",
    "![alt text](imgs/wide-n-deep.png \"Wide \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement adaptive learning rate\n",
    "* [exponential_decay](https://www.tensorflow.org/api_docs/python/tf/train/exponential_decay)\n",
    "* [consine_decay](https://www.tensorflow.org/api_docs/python/tf/train/cosine_decay)\n",
    "* [linear_cosine_decay](https://www.tensorflow.org/api_docs/python/tf/train/linear_cosine_decay)\n",
    "* [consine_decay_restarts](https://www.tensorflow.org/api_docs/python/tf/train/cosine_decay_restarts)\n",
    "* [polynomial decay](https://www.tensorflow.org/api_docs/python/tf/train/polynomial_decay)\n",
    "* [piecewise_constant_decay](https://www.tensorflow.org/api_docs/python/tf/train/piecewise_constant_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_optimizer(initial_learning_rate, decay_steps):\n",
    "    learning_rate = tf.train.cosine_decay_restarts(\n",
    "        initial_learning_rate,\n",
    "        tf.train.get_global_step(),\n",
    "        first_decay_steps=50,\n",
    "        t_mul=2.0,\n",
    "        m_mul=1.0,\n",
    "        alpha=0.0,\n",
    "    )\n",
    "    \n",
    "    tf.summary.scalar('learning_rate', learning_rate)\n",
    "    return tf.train.AdamOptimizer(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add an evaluation metric\n",
    "* [tf.metrics](https://www.tensorflow.org/api_docs/python/tf/metrics)\n",
    "* [tf.estimator.add_metric](https://www.tensorflow.org/api_docs/python/tf/estimator/add_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_fn(labels, predictions):\n",
    "    \n",
    "    metrics = {}\n",
    "    label_index = tf.contrib.lookup.index_table_from_tensor(tf.constant(TARGET_LABELS)).lookup(labels)\n",
    "    one_hot_labels = tf.one_hot(label_index, len(TARGET_LABELS))\n",
    "    \n",
    "    metrics['mirco_accuracy'] = tf.metrics.mean_per_class_accuracy(\n",
    "        labels=label_index,\n",
    "        predictions=predictions['class_ids'],\n",
    "        num_classes=2\n",
    "    )\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_estimator(params, run_config):\n",
    "    \n",
    "    wide_columns, deep_columns = create_feature_columns()\n",
    "    \n",
    "    estimator = tf.estimator.DNNLinearCombinedClassifier(\n",
    "\n",
    "        n_classes=len(TARGET_LABELS),\n",
    "        label_vocabulary=TARGET_LABELS,\n",
    "        weight_column=WEIGHT_COLUMN_NAME,\n",
    "\n",
    "        dnn_feature_columns=deep_columns,\n",
    "        dnn_optimizer=lambda: update_optimizer(\n",
    "            params.learning_rate, params.max_steps),\n",
    "#         dnn_optimizer=tf.train.AdamOptimizer(\n",
    "#           learning_rate=params.learning_rate),\n",
    "        dnn_hidden_units=params.hidden_units,\n",
    "        dnn_dropout=params.dropout,\n",
    "        dnn_activation_fn=tf.nn.relu,\n",
    "        batch_norm=True,\n",
    "\n",
    "        linear_feature_columns=wide_columns,\n",
    "        linear_optimizer='Ftrl',\n",
    "\n",
    "        config=run_config\n",
    "    )\n",
    "    \n",
    "    estimator = tf.estimator.add_metrics(\n",
    "        estimator, metric_fn)\n",
    "    \n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run a Train and Evaluate Experiment\n",
    "**Delete** the **model_dir** file if you don't want a **Warm Start**\n",
    "* If not deleted, and you **alter** the model, it will error.\n",
    "\n",
    "[TrainSpec](https://www.tensorflow.org/api_docs/python/tf/estimator/TrainSpec)\n",
    "* Set **shuffle** in the **input_fn** to **True**\n",
    "* Set **num_epochs** in the **input_fn** to **None**\n",
    "* Set **max_steps**. One batch (feed-forward pass & backpropagation) \n",
    "corresponds to 1 training step. \n",
    "\n",
    "[EvalSpec](https://www.tensorflow.org/api_docs/python/tf/estimator/EvalSpec)\n",
    "* Set **shuffle** in the **input_fn** to **False**\n",
    "* Set Set **num_epochs** in the **input_fn** to **1**\n",
    "* Set **steps** to **None** if you want to use all the evaluation data. \n",
    "* Otherwise, set **steps** to the number of batches you want to use for evaluation, and set **shuffle** to True.\n",
    "* Set **start_delay_secs** to 0  to start evaluation as soon as a checkpoint is produced.\n",
    "* Set **throttle_secs** to 0 to re-evaluate as soon as a new checkpoint is produced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Implement an experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(estimator, params, run_config, \n",
    "                   resume=False, train_hooks=None):\n",
    "    \n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    \n",
    "    if not resume: \n",
    "        if tf_io.gfile.exists(run_config.model_dir):\n",
    "            print(\"Removing previous model checkpoints...\")\n",
    "            tf_io.gfile.rmtree(run_config.model_dir)\n",
    "    else:\n",
    "        print(\"Resuming training...\")\n",
    "\n",
    "    # Create train specs.\n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "        input_fn = make_input_fn(\n",
    "            TRANSFORMED_TRAIN_DATA_FILE,\n",
    "            batch_size=params.batch_size,\n",
    "            num_epochs=None, # Run until the max_steps is reached.\n",
    "            shuffle=True\n",
    "        ),\n",
    "        max_steps=params.max_steps,\n",
    "        hooks=train_hooks\n",
    "    )\n",
    "\n",
    "    # Create eval specs.\n",
    "    eval_spec = tf.estimator.EvalSpec(\n",
    "        input_fn = make_input_fn(\n",
    "            TRANSFORMED_EVAL_DATA_FILE,\n",
    "            batch_size=params.batch_size, \n",
    "        ),\n",
    "        exporters=None, # This can be set to export a saved model \n",
    "        start_delay_secs=0,\n",
    "        throttle_secs=0,\n",
    "        steps=None # Set to limit number of steps for evaluation.\n",
    "    )\n",
    "  \n",
    "\n",
    "    print(\"Experiment started...\")\n",
    "    print(\".......................................\")\n",
    "  \n",
    "    # Run train and evaluate.\n",
    "    tf.estimator.train_and_evaluate(\n",
    "        estimator=estimator,\n",
    "        train_spec=train_spec, \n",
    "        eval_spec=eval_spec\n",
    "    )\n",
    "\n",
    "    print(\".......................................\")\n",
    "    print(\"Experiment finished.\")\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters and run_config.\n",
    "\n",
    "* Set **model_dir** in the **run_config**\n",
    "* If the data **size is known**, training **steps**, with respect to **epochs** would be: **(training_size / batch_size) * epochs** \n",
    "* By default, a **checkpoint** is saved every 600 secs.  That is, the model is **evaluated** only every 10mins. \n",
    "* To change this behaviour, set one of the following parameters in the **run_config**\n",
    " * **save_checkpoints_secs**: Save checkpoints every this many **seconds**.\n",
    " * **save_checkpoints_steps**: Save checkpoints every this many **steps**.\n",
    "* Set the number of the checkpoints to keep using **keep_checkpoint_max**\n",
    "* Set **train_distribute** and/or **eval_dsitribute** strategy for Multi-GPU training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 19820610\n",
    "\n",
    "class Parameters():\n",
    "    pass\n",
    "\n",
    "MODEL_NAME = 'dnn_classifier'\n",
    "MODEL_DIR = os.path.join(MODELS_DIR, MODEL_NAME)\n",
    "\n",
    "TRAIN_DATA_SIZE = 32561\n",
    "\n",
    "params = Parameters()\n",
    "params.learning_rate = 0.001\n",
    "params.hidden_units = [128, 128, 128]\n",
    "params.dropout = 0.15\n",
    "params.batch_size =  128\n",
    "\n",
    "# Set number of steps with respect to epochs.\n",
    "epochs = 1000\n",
    "steps_per_epoch = int(math.ceil(TRAIN_DATA_SIZE / params.batch_size))\n",
    "params.max_steps = steps_per_epoch * epochs\n",
    "\n",
    "run_config = tf.estimator.RunConfig(\n",
    "    tf_random_seed=RANDOM_SEED,\n",
    "    save_checkpoints_steps=steps_per_epoch, # Save a checkpoint after each epoch, evaluate the model after each epoch.\n",
    "    keep_checkpoint_max=3, # Keep the 3 most recently  produced checkpoints.\n",
    "    model_dir=MODEL_DIR,\n",
    "    save_summary_steps=100, # Summary steps for Tensorboard.\n",
    "    log_step_count_steps=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Run experiment with early stopping hook\n",
    "* [stop_if_higher_hook](https://www.google.com/search?q=stop_if_higher_hook&oq=stop_if_higher_hook) \n",
    "* [stop_if_lower_hook](https://www.tensorflow.org/api_docs/python/tf/contrib/estimator/stop_if_lower_hook) \n",
    "* [stop_if_no_increase_hook](https://www.tensorflow.org/api_docs/python/tf/contrib/estimator/stop_if_no_increase_hook)\n",
    "* [stop_if_no_decrease_hook](https://www.tensorflow.org/api_docs/python/tf/contrib/estimator/stop_if_no_decrease_hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimator = create_estimator(params, run_config)\n",
    "\n",
    "early_stopping_hook = tf.estimator.experimental.stop_if_no_increase_hook(\n",
    "    estimator,\n",
    "    'accuracy',\n",
    "    max_steps_without_increase=100,\n",
    "    run_every_secs=None,\n",
    "    run_every_steps=500\n",
    ")\n",
    "\n",
    "%time run_experiment(estimator, params, run_config, train_hooks=[early_stopping_hook])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Export the Model for Serving "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Implement a serving_input_receive_fn\n",
    "This function expect **raw** data interface, then it applies the **transformation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "\n",
    "def _serving_input_receiver_fn():\n",
    "    \n",
    "    source_raw_schema = tfdv.load_schema_text(RAW_SCHEMA_LOCATION)\n",
    "    raw_feature_spec = schema_utils.schema_as_feature_spec(source_raw_schema).feature_spec\n",
    "    raw_feature_spec.pop(TARGET_FEATURE_NAME)\n",
    "    raw_feature_spec.pop(WEIGHT_COLUMN_NAME)\n",
    "\n",
    "    # Create the interface for the serving function with the raw features\n",
    "    raw_features = tf.estimator.export.build_parsing_serving_input_receiver_fn(\n",
    "        raw_feature_spec)().features\n",
    "\n",
    "    receiver_tensors = {\n",
    "        feature: tf.placeholder(shape=[None], dtype=raw_features[feature].dtype) \n",
    "        for feature in raw_features\n",
    "    }\n",
    "\n",
    "    receiver_tensors_expanded = {\n",
    "        tensor: tf.reshape(receiver_tensors[tensor], (-1, 1)) \n",
    "        for tensor in receiver_tensors\n",
    "    }\n",
    "\n",
    "    # Apply the transform function \n",
    "    transformed_features = transform_output.transform_raw_features(\n",
    "        receiver_tensors_expanded)\n",
    "\n",
    "    return tf.estimator.export.ServingInputReceiver(\n",
    "        transformed_features, receiver_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Export SavedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "export_dir = os.path.join(MODEL_DIR, 'export')\n",
    "\n",
    "if tf.gfile.Exists(export_dir):\n",
    "    tf.gfile.DeleteRecursively(export_dir)\n",
    "        \n",
    "saved_model_location = estimator.export_savedmodel(\n",
    "    export_dir_base=export_dir,\n",
    "    serving_input_receiver_fn=_serving_input_receiver_fn\n",
    ")\n",
    "\n",
    "print(saved_model_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!saved_model_cli show --dir=${saved_model_location} --all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_fn = tf.contrib.predictor.from_saved_model(\n",
    "    export_dir = saved_model_location,\n",
    "    signature_def_key=\"predict\"\n",
    ")\n",
    "\n",
    "output = predictor_fn(\n",
    "    {\n",
    "        'age': [34.0],\n",
    "        'workclass': ['Private'],\n",
    "        'education': ['Doctorate'],\n",
    "        'education_num': [10.0],\n",
    "        'marital_status': ['Married-civ-spouse'],\n",
    "        'occupation': ['Prof-specialty'],\n",
    "        'relationship': ['Husband'],\n",
    "        'race': ['White'],\n",
    "        'gender': ['Male'],\n",
    "        'capital_gain': [0.0], \n",
    "        'capital_loss': [0.0], \n",
    "        'hours_per_week': [40.0],\n",
    "        'native_country':['England']\n",
    "    }\n",
    ")\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seralizing estimator object to be used in the following lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(estimator, './estimator.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q tensorflow-model-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_analysis as tfma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Implement a serving_input_receive_fn\n",
    "This function expect **transform** data interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_input_receiver_fn():\n",
    "    \n",
    "    source_raw_schema = tfdv.load_schema_text(RAW_SCHEMA_LOCATION)\n",
    "    raw_feature_spec = schema_utils.schema_as_feature_spec(source_raw_schema).feature_spec\n",
    "    serving_input_receiver = tf.estimator.export.build_parsing_serving_input_receiver_fn(\n",
    "        raw_feature_spec, default_batch_size=None)()\n",
    "    \n",
    "    features = serving_input_receiver.features.copy()\n",
    "    transformed_features = transform_output.transform_raw_features(features)\n",
    "\n",
    "    features.update(transformed_features)\n",
    "    \n",
    "    return tfma.export.EvalInputReceiver(\n",
    "        features=features,\n",
    "        receiver_tensors=serving_input_receiver.receiver_tensors,\n",
    "        labels=transformed_features[TARGET_FEATURE_NAME]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model_dir = os.path.join(MODEL_DIR, \"export/evaluate\")\n",
    "if tf_io.gfile.exists(eval_model_dir):\n",
    "    tf_io.gfile.rmtree(eval_model_dir)\n",
    "\n",
    "tfma.export.export_eval_savedmodel(\n",
    "        estimator=estimator,\n",
    "        export_dir_base=eval_model_dir,\n",
    "        eval_input_receiver_fn=eval_input_receiver_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(estimator, './estimator.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = joblib.load( './estimator.joblib')\n",
    "tfma.export.export_eval_savedmodel(\n",
    "        estimator=e,\n",
    "        export_dir_base=eval_model_dir,\n",
    "        eval_input_receiver_fn=eval_input_receiver_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Step: TFMA\n",
    "Model Validation with TensorFlow Model Analysis (TFMA)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
